{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.optim import lr_scheduler\n",
    "import itertools\n",
    "from torch import optim \n",
    "import random \n",
    "import os \n",
    "from PIL import Image \n",
    "import torch.utils.data as data\n",
    "import os.path\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets,transforms,utils\n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "from IPython import display\n",
    "import numpy\n",
    "import random\n",
    "from tkinter import *\n",
    "from tkinter.messagebox import showinfo\n",
    "import pdb\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that defines the structure of the dataset\n",
    "# Sets up the file paths to each style domain image folder\n",
    "# Transforms and prepares each image for enumeration\n",
    "# Creates an indexing system for the dataset \n",
    "\n",
    "class DatasetStructure():\n",
    "    \n",
    "    def initialize(self,path,loadSize,fineSize,color):\n",
    "        \n",
    "        self.dir_A = os.path.join(path, 'train' + 'A')\n",
    "        self.dir_B = os.path.join(path, 'train' + 'B')\n",
    "        \n",
    "        self.A_paths = []\n",
    "    \n",
    "        for root, _, files in sorted(os.walk(self.dir_A)):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    path = os.path.join(root, file)\n",
    "                    self.A_paths.append(path)\n",
    "                    \n",
    "        self.B_paths = []\n",
    "    \n",
    "        for root, _, files in sorted(os.walk(self.dir_B)):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    path = os.path.join(root, file)\n",
    "                    self.B_paths.append(path)                 \n",
    "\n",
    "        self.A_paths = sorted(self.A_paths)\n",
    "        self.B_paths = sorted(self.B_paths)\n",
    "        \n",
    "        self.A_size = len(self.A_paths)\n",
    "        self.B_size = len(self.B_paths)\n",
    "        \n",
    "        self.transform = get_transform(loadSize,fineSize,color) \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        A_path = self.A_paths[index % self.A_size]\n",
    "\n",
    "        index_B = index % self.B_size\n",
    "\n",
    "        B_path = self.B_paths[index_B]\n",
    "        A_img = Image.open(A_path).convert('RGB')\n",
    "        B_img = Image.open(B_path).convert('RGB')\n",
    "\n",
    "        A = self.transform(A_img)\n",
    "        B = self.transform(B_img)\n",
    "\n",
    "        return {'A': A, 'B': B,\n",
    "                'A_paths': A_path, 'B_paths': B_path}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(self.A_size, self.B_size)\n",
    "\n",
    "# Data set creation functions\n",
    "\n",
    "# Function that creates a list of transforms from the Pytorch library\n",
    "# Each image in the dataset is passed through the transforms\n",
    "# Transforms include: Transfer to Tensor data structure, random crops, and normalization\n",
    "\n",
    "def get_transform(load_size,crop_size,color):\n",
    "    \n",
    "    transform_list = [transforms.Resize([load_size,load_size],Image.BICUBIC),transforms.RandomCrop(crop_size),\n",
    "                      transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))]\n",
    "    \n",
    "    if color == 'grayscale':\n",
    "        transform_list.append(transforms.Grayscale(num_output_channels = 3))\n",
    "        \n",
    "    composed_transforms = transforms.Compose(transform_list)\n",
    "    \n",
    "    return composed_transforms  \n",
    "    \n",
    "# Creates an instance of the dataset\n",
    "# Specifies the image sizes in the dataset and whether they are gray or color\n",
    "\n",
    "def createDataset(path,load_size,crop_size,color):\n",
    "    \n",
    "    instance = DatasetStructure()\n",
    "    instance.initialize(path,load_size,crop_size,color)\n",
    "    \n",
    "    dataset = instance\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=not 'store_true', num_workers=int(0))\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "# Function that gets a chosen number of images from the dataset in order to display the network training progress\n",
    "\n",
    "def get_test_dataset(num_imgs,dataset):\n",
    "    \n",
    "    a = 0\n",
    "    tensor_list_A = []\n",
    " \n",
    "    for i, data in enumerate(dataset): \n",
    "        real_A = data['A']\n",
    "        tensor_list_A.append(real_A[0])\n",
    "       \n",
    "        a +=1\n",
    "        if a == num_imgs:\n",
    "            break\n",
    "        \n",
    "    img_list = torch.stack(tensor_list_A) \n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network functions\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        \n",
    "        NN_sequence = [nn.Conv2d(3,64,kernel_size=4,stride=2,padding=1),nn.LeakyReLU(0.2,True),\n",
    "                    \n",
    "                    nn.Conv2d(64, 64 * 2,kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "                    norm_layer(64 * 2),nn.LeakyReLU(0.2, True),\n",
    "                    \n",
    "                    nn.Conv2d(64 * 2, 64 * 4,kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "                    norm_layer(64 * 4),nn.LeakyReLU(0.2, True),\n",
    "                    \n",
    "                    nn.Conv2d(64 * 4, 64 * 8,kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "                    norm_layer(64 * 8),nn.LeakyReLU(0.2, True),\n",
    "                    \n",
    "                    nn.Conv2d(64 * 8, 64 * 8,kernel_size=4, stride=1, padding=1, bias=use_bias),\n",
    "                    norm_layer(64 * 8),nn.LeakyReLU(0.2, True),\n",
    "                    \n",
    "                    nn.Conv2d(64 * 8, 1, kernel_size=4, stride=1, padding=1)]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            NN_sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*NN_sequence)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input) \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=9, padding_type='reflect'):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d \n",
    "            \n",
    "        encoder = [nn.ReflectionPad2d(3),nn.Conv2d(3, 64, kernel_size=7, padding=0,bias=use_bias),norm_layer(64),nn.ReLU(True), \n",
    "                 \n",
    "                 nn.Conv2d(64, 64 * 2, kernel_size=3,stride=2, padding=1, bias=use_bias),\n",
    "                 norm_layer(64 * 2),nn.ReLU(True),\n",
    "                 \n",
    "                 nn.Conv2d(64 * 2, 64 * 4, kernel_size=3,stride=2, padding=1, bias=use_bias),\n",
    "                 norm_layer(64 * 4),nn.ReLU(True)]\n",
    "        \n",
    "        transformer = []\n",
    "        for i in range(n_blocks):\n",
    "            transformer += [ResnetBlock(64 * 4, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, \n",
    "                                  use_bias=use_bias)]           \n",
    "       \n",
    "        decoder = [nn.ConvTranspose2d(64 * 4, int(64 * 4 / 2),kernel_size=3, stride=2,padding=1, output_padding=1,\n",
    "                                      bias=use_bias),norm_layer(int(64 * 4 / 2)),nn.ReLU(True), \n",
    "                   \n",
    "                   nn.ConvTranspose2d(64 * 2, int(64 * 2 / 2),kernel_size=3, stride=2,padding=1, output_padding=1,\n",
    "                                      bias=use_bias),norm_layer(int(64 * 2 / 2)),nn.ReLU(True),\n",
    "                   \n",
    "                   nn.ReflectionPad2d(3),\n",
    "                   \n",
    "                   nn.Conv2d(64, 3, kernel_size=7, padding=0),\n",
    "                   \n",
    "                   nn.Tanh()]\n",
    "\n",
    "        model = encoder + transformer + decoder\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    \n",
    "# Define a resnet block\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        \n",
    "        self.block = self.build_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "\n",
    "        block = [nn.ReflectionPad2d(1),\n",
    "                       \n",
    "                 nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=use_bias),norm_layer(dim),nn.ReLU(True)]\n",
    "        \n",
    "        if use_dropout:\n",
    "            block += [nn.Dropout(0.5)]\n",
    "\n",
    "        block += [nn.ReflectionPad2d(1),\n",
    "                       \n",
    "                  nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=use_bias),norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.block(x)\n",
    "        return out\n",
    "    \n",
    "# initializes NN weights based on their classname\n",
    "# Convolution and Linear classes given weights from normalized distribution with mean 0.0 and standard deviation 0.03\n",
    "# Batch norm class given weights from normalized distribution with mean 0.9 and standard deviation 0.03\n",
    "# Bias weights are set at a constant 0\n",
    "\n",
    "def normal_weight_init(NN):\n",
    "    def weight_assign(m):\n",
    "        classname = m.__class__.__name__       \n",
    "        # Find the weights of the convolution and linear classes and assign a normal weight distribution        \n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            init.normal_(m.weight.data, 0.0, 0.03)            \n",
    "            # Set bias to 0.0           \n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 0.9, 0.03)\n",
    "            init.constant_(m.bias.data, 0.0)            \n",
    "    NN.apply(weight_assign)  \n",
    "\n",
    "# Creates all of the models required for CycleGAN training\n",
    "# Includes Discriminators for each domains (DA and DB)\n",
    "# as well as the Generator transforming domain A into domain B\n",
    "# and the generator transforming domain B into domain A\n",
    "    \n",
    "def createModels():\n",
    "    norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
    "    \n",
    "    DA = Discriminator(norm_layer, 'store_true')\n",
    "    DA.to('cuda')  \n",
    "    normal_weight_init(DA)\n",
    "    \n",
    "    DB = Discriminator(norm_layer, 'store_true')\n",
    "    DB.to('cuda')  \n",
    "    normal_weight_init(DB)\n",
    "    \n",
    "    GAB = Generator(norm_layer, not 'store_true', n_blocks=9)\n",
    "    GAB.to('cuda')  \n",
    "    normal_weight_init(GAB)\n",
    "    \n",
    "    GBA = Generator(norm_layer, not 'store_true', n_blocks=9)\n",
    "    GBA.to('cuda')  \n",
    "    normal_weight_init(GBA)\n",
    "        \n",
    "    return GAB,GBA,DA,DB\n",
    "\n",
    "def createNN(NN_type,gpu):\n",
    "    \n",
    "    norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
    "    \n",
    "    if NN_type == 'generator':\n",
    "        NN = Generator(norm_layer, not 'store_true', n_blocks=9)\n",
    "\n",
    "    elif NN_type == 'discriminator':\n",
    "        NN = Discriminator(norm_layer, 'store_true')\n",
    "      \n",
    "    if gpu:\n",
    "        NN.to('cuda')  \n",
    "    else:\n",
    "        NN.to('cpu')  \n",
    "        \n",
    "    normal_weight_init(NN)\n",
    "        \n",
    "    return NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "\n",
    "def update_pool(imgs,pool):\n",
    "    pool_imgs = []\n",
    "    for img in imgs:\n",
    "        img = torch.unsqueeze(img.data, 0)\n",
    "        if pool[1] < pool[0]:\n",
    "            pool[1] = pool[1] + 1\n",
    "            pool[2].append(img)\n",
    "            pool_imgs.append(img)\n",
    "        else:\n",
    "            if random.uniform(0, 1) > 0.5:\n",
    "                r_id = random.randint(0, pool[0] - 1)\n",
    "                temp_img = pool[2][r_id].clone()\n",
    "                pool[2][r_id] = img\n",
    "                pool_imgs.append(temp_img)\n",
    "            else:\n",
    "                pool_imgs.append(img)\n",
    "    pool_imgs = torch.cat(pool_imgs, 0)\n",
    "    return pool_imgs\n",
    "\n",
    "def gan_loss(input,target):\n",
    "    \n",
    "    loss = nn.BCELoss()\n",
    "        \n",
    "    if target:\n",
    "        target_tensor = torch.tensor(1.0)\n",
    "    else:\n",
    "        target_tensor = torch.tensor(0.0)\n",
    "        \n",
    "    target_tensor = target_tensor.expand_as(input)\n",
    "    target_tensor = target_tensor.cuda()\n",
    "        \n",
    "    return loss(input,target_tensor)\n",
    "\n",
    "# learning rate functions\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "\n",
    "    def lambda_rule(epoch):\n",
    "        lr_l = 1.0 - max(0, epoch - 99) / float(101)\n",
    "        return lr_l\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "\n",
    "    return scheduler\n",
    "        \n",
    "def update_learning_rate(schedulers,optimizers):\n",
    "    for scheduler in schedulers:\n",
    "        scheduler.step()\n",
    "    lr = optimizers[0].param_groups[0]['lr']\n",
    "    \n",
    "def createOptimizers():\n",
    "      \n",
    "    optimizer_G = torch.optim.Adam(itertools.chain(GAB.parameters(), GBA.parameters()),0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(itertools.chain(DA.parameters(), DB.parameters()),0.0002, betas=(0.5, 0.999))\n",
    "    optimizers = [optimizer_G,optimizer_D]\n",
    "    \n",
    "    return optimizers\n",
    "\n",
    "def optimize_parameters(real_A,real_B,optimizers):\n",
    "    \n",
    "    lf = torch.nn.L1Loss()\n",
    "    \n",
    "    fake_B = GAB(real_A)\n",
    "    re_A = GBA(fake_B)\n",
    "    fake_A = GBA(real_B)\n",
    "    re_B = GAB(fake_A)\n",
    "    \n",
    "    nets = [DA, DB]\n",
    "    requires_grad = False    \n",
    "    for net in nets:\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad = requires_grad           \n",
    "    optimizers[0].zero_grad()\n",
    "    \n",
    "    g_loss = (gan_loss(DA(fake_B), True)) + (gan_loss(DB(fake_A), True)) + (lf(re_A, real_A) * 10.0) + \\\n",
    "             (lf(re_B, real_B) * 10.0) + (lf(GAB(real_B), real_B) * 10.0 * 0.5) + \\\n",
    "             (lf(GBA(real_A), real_A) * 10.0 * 0.5)\n",
    "    \n",
    "    g_loss.backward()\n",
    "    \n",
    "    optimizers[0].step() \n",
    "        \n",
    "    nets = [DA, DB]\n",
    "    requires_grad = True   \n",
    "    for net in nets:\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad = requires_grad            \n",
    "    optimizers[1].zero_grad()  \n",
    "    \n",
    "    # calculate discrimintor A loss, by predicting real images and generator AB fake images\n",
    "     \n",
    "    fake_B = update_pool(fake_B,B_pool)    \n",
    "    pred_real_DA = DA(real_B)\n",
    "    pred_fake_DA = DA(fake_B.detach())       \n",
    "    DA_loss = ((gan_loss(pred_real_DA, True)) + (gan_loss(pred_fake_DA, False))) * 0.5\n",
    "    DA_loss.backward() \n",
    "    \n",
    "    # calculate discrimintor B loss, by predicting real images and generator AB fake images    \n",
    "    \n",
    "    fake_A = update_pool(fake_A,A_pool)   \n",
    "    pred_real_DB = DB(real_A)    \n",
    "    pred_fake_DB = DB(fake_A.detach())    \n",
    "    DB_loss = ((gan_loss(pred_real_DB, True)) + (gan_loss(pred_fake_DB, False))) * 0.5\n",
    "    DB_loss.backward()\n",
    "    \n",
    "    optimizers[1].step()  \n",
    "    \n",
    "def startTraining(epoch_limit,in_size,out_size,sample_num,path,save_image_path,schedulers,optimizers):\n",
    "\n",
    "    # create dataset\n",
    "\n",
    "    dataset = createDataset(train_path,in_size,out_size,'color')\n",
    "    img_list = get_test_dataset(sample_num,dataset)\n",
    "        \n",
    "    # training process\n",
    "\n",
    "    for epoch in range(1,epoch_limit):\n",
    "        i = 0\n",
    "        for i, data in enumerate(dataset):\n",
    "        \n",
    "            real_A = data['A']\n",
    "            real_B = data['B']\n",
    "            if torch.cuda.is_available(): real_A = real_A.cuda()\n",
    "            if torch.cuda.is_available(): real_B = real_B.cuda()\n",
    "  \n",
    "            optimize_parameters(real_A,real_B,optimizers)\n",
    "        \n",
    "            # Display Progress\n",
    "            if (i) % 100 == 0:\n",
    "                display.clear_output(True)\n",
    "                # Display Images\n",
    "                test_images_B = GAB(img_list.cuda()).data.cpu()\n",
    "                        \n",
    "                saveImages(save_image_path,test_images_B,epoch,i,'cyclegan','test_save',i)\n",
    "\n",
    "                print (i)\n",
    "                print ('epoch: ' ,epoch,'/',200)     \n",
    "\n",
    "                saveModel(GAB,epoch,str(i),path)\n",
    "            i = i + 1\n",
    "        \n",
    "        update_learning_rate(schedulers,optimizers)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger functions\n",
    "\n",
    "# Save trained models and images functions\n",
    "\n",
    "def saveModel(NN,epoch,batch,path):\n",
    "    \n",
    "    torch.save(NN.state_dict(),'{}/NN_epoch_{}_{}'.format(path, epoch,batch))\n",
    "\n",
    "def saveImages(save_image_path,images,epoch,n_batch,model_name,data_name,num_batches):\n",
    "    \n",
    "    # Make horizontal grid from image tensor\n",
    "    horizontal_grid = vutils.make_grid(images, True, scale_each=True)\n",
    "    \n",
    "    # Add horizontal images to tensorboard\n",
    "    step = epoch * num_batches + n_batch\n",
    "    comment = '{}_{}'.format(model_name, data_name)\n",
    "    img_name = '{}/images{}'.format(comment, '')\n",
    "    writer = SummaryWriter(comment)\n",
    "    writer.add_image(img_name, horizontal_grid, step)\n",
    "    \n",
    "    # Plot and save horizontal\n",
    "    fig = plt.figure(figsize=(16, 16))\n",
    "    plt.imshow(np.moveaxis(horizontal_grid.numpy(), 0, -1))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "        \n",
    "    fig.savefig('{}/epoch_{}_batch_{}.png'.format(save_image_path,epoch, n_batch))\n",
    "    plt.close()\n",
    "    \n",
    "# function that saves image list to a file\n",
    "\n",
    "def saveImgList(img_list,save_path2,name):\n",
    "    for i in range(0,len(img_list)):\n",
    "        utils.save_image(img_list[i], save_path2 + name +'img.jpg',normalize = True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Classifier Functions\n",
    "\n",
    "def optimize_classifier(classA,classB,D,optimizer_D):\n",
    "  \n",
    "    nets = [D]\n",
    "    requires_grad = True    \n",
    "    for net in nets:\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad = requires_grad                                \n",
    "    optimizer_D.zero_grad()\n",
    "    \n",
    "    pred_real_D = D(classA)\n",
    "    pred_fake_D = D(classB)    \n",
    "    D_loss = ((gan_loss(pred_real_D, True)) + (gan_loss(pred_fake_D, False))) * 0.5   \n",
    "    D_loss.backward()\n",
    "        \n",
    "    optimizer_D.step()\n",
    "    \n",
    "def createClassifier(epoch_limit,data_path,save_classifier_path,size,color):\n",
    "    \n",
    "    D = createNN('discriminator',True)\n",
    "    opt_D = torch.optim.Adam(itertools.chain(D.parameters()),0.0002, betas=(0.5, 0.999))\n",
    "    schedulers = [get_scheduler(opt_D)]\n",
    "    \n",
    "    dataset = createDataset(data_path,size,size,color)\n",
    "\n",
    "    epoch = 1\n",
    "    for epoch in range(epoch, epoch_limit):\n",
    "        i=0\n",
    "        for i, data in enumerate(dataset):\n",
    "            real_A = data['A']\n",
    "            real_B = data['B']\n",
    "     \n",
    "            if torch.cuda.is_available(): real_A = real_A.cuda()\n",
    "            if torch.cuda.is_available(): real_B = real_B.cuda()\n",
    "  \n",
    "            optimize_classifier(real_A,real_B,D,opt_D)\n",
    "        \n",
    "            # Display Progress\n",
    "            if (i) % 100 == 0:\n",
    "                display.clear_output(True)\n",
    "                print (i)\n",
    "                print ('epoch: ' ,epoch,'/',epoch_limit) \n",
    "                            \n",
    "            i = i + 1\n",
    "            \n",
    "        saveModel(D,epoch,'cls',save_classifier_path)\n",
    "            \n",
    "        update_learning_rate(schedulers,[opt_D])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Genetic Algorithm Functions\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function that uploads and prepares a batch of images from a specified path\n",
    "# Parameters decide the size of the images and whether they are color or gray\n",
    "\n",
    "def get_images(path,size,color):\n",
    "    \n",
    "    test_dataset = createDataset(path,size,size,color)\n",
    "\n",
    "    tensor_list = []    \n",
    "    for i, data in enumerate(test_dataset):\n",
    "        real_A = data['A']\n",
    "        real_B = data['B']\n",
    "        real = [real_A,real_B]\n",
    "        tensor_list.append(real[0][0])     \n",
    " \n",
    "    img_data = torch.stack(tensor_list)   \n",
    "    return img_data\n",
    "\n",
    "# String together multiple random NNs into a chain by placing them into a list \n",
    "\n",
    "def create_NN_chain(chain_size, noiseNN_limit,colorNN_limit):\n",
    "    \n",
    "    NN_chain = []\n",
    "    chain_num = ''\n",
    "    \n",
    "    for i in range(1,chain_size):\n",
    "        \n",
    "        # Randomly select NN index\n",
    "        \n",
    "        colorNN_choice = (random.randint(1,colorNN_limit))\n",
    "        noiseNN_choice = (random.randint(1,noiseNN_limit))\n",
    "        desc = ''\n",
    "        \n",
    "        # Only first NN of the chain converts noise to color image\n",
    "        \n",
    "        if i == 1:\n",
    "            model_path = n2c_NN_path + '(' + str(noiseNN_choice) + ')'\n",
    "            desc = str(noiseNN_choice)            \n",
    "        else:            \n",
    "            model_path = c2r_NN_path + '(' + str(colorNN_choice) + ')'\n",
    "            desc = str(colorNN_choice)\n",
    "         \n",
    "        # Instantiate selected NN taken from the chosen path and append it to the chain\n",
    "        \n",
    "        NN = createNN('generator',True)\n",
    "        NN.load_state_dict(torch.load(model_path,map_location = 'cuda'))       \n",
    "        NN_chain.append(NN)        \n",
    "        chain_num = chain_num + '-'+ desc\n",
    "        \n",
    "    return NN_chain,chain_num\n",
    "\n",
    "# Functions that pass data through the NN chain and return resulting images\n",
    "def NNPass(noise_data, NN_chain,chain_path):\n",
    "    \n",
    "    score_image_list = []\n",
    "    \n",
    "    for img in noise_data:\n",
    "\n",
    "        for i in range(0,len(NN_chain)):\n",
    "            if i == len(NN_chain):\n",
    "                end = True \n",
    "            else:\n",
    "                end = False\n",
    "                \n",
    "            img = chainPass(NN_chain,i,img,chain_path,end)\n",
    "            \n",
    "        score_image_list.append(img)\n",
    "        \n",
    "    return score_image_list\n",
    "\n",
    "def chainPass(NN_chain,chain_idx,img,chain_path,end):\n",
    "    img1 = torch.tensor(img)\n",
    "    img2 = torch.stack((img1,img1))      \n",
    "    img3 = NN_chain[chain_idx](img2.cuda()).data.cpu()\n",
    "    img4 = img3[0]\n",
    "               \n",
    "    utils.save_image(img3[0], chain_path+'img1.jpg',normalize = True)\n",
    "               \n",
    "    nd = get_images(reupload_path,256,'color')\n",
    "               \n",
    "    img5 = nd[0]\n",
    "    \n",
    "    if end:\n",
    "        return img4\n",
    "    else:\n",
    "        return img5\n",
    "\n",
    "# function that scores a list of images with the discriminator fitness function\n",
    "\n",
    "def scoreImgList(path,classifier,color):\n",
    "    \n",
    "    img_list = get_images(path,256,color)\n",
    "    \n",
    "    img_scores =[]\n",
    "    for i in img_list:\n",
    "        img = torch.tensor(i)\n",
    "        img2 = torch.stack((img,img))\n",
    "        img_score = classifier(img2.cuda()).data.cpu()\n",
    "        img_scores.append((torch.mean(img_score).numpy())*-1)\n",
    "        \n",
    "        #print((torch.mean(img_score).numpy()))\n",
    "\n",
    "    # avg is the score of the NN chain \n",
    "\n",
    "    img_score_avg = sum(img_scores)/len(img_scores)\n",
    "        \n",
    "    return(img_score_avg) \n",
    "\n",
    "# function that creates a population of NN chains\n",
    "\n",
    "def createPopulation(noise_data,save_path,population_size,chain_size):\n",
    "\n",
    "    pop = []\n",
    "    scores = []\n",
    "    pop_desc = []\n",
    "    \n",
    "    for i in range(0,population_size):\n",
    "        \n",
    "        NN_chain,chain_num = create_NN_chain(chain_size,5,5)    \n",
    "        pop.append(NN_chain)\n",
    "    \n",
    "        new_img_list = NNPass(noise_data,NN_chain,chain_path)\n",
    "\n",
    "        # save out images from NN chain\n",
    "\n",
    "        saveImgList(new_img_list,save_path,'') \n",
    "        \n",
    "        # Uses classifier to get score of each image in the img_list created by the NN chain\n",
    "\n",
    "        NN_chain_score = scoreImgList(score_path,classifier,'grayscale')\n",
    "        \n",
    "        scores.append(NN_chain_score)\n",
    "        pop_desc.append(chain_num)\n",
    "        \n",
    "    return pop,scores,pop_desc\n",
    "\n",
    "# function to pick two members of the population and compare their scores, outputs winner and loser\n",
    "\n",
    "def getWinnerLoser(popul,scores,kd):\n",
    "    population_size = len(popul)\n",
    "\n",
    "    # Choose an index1 from 0 to 9 and an index2 a certain distance from index1\n",
    "\n",
    "    id1 = random.randint(0,population_size-1)\n",
    "    d = random.randint(0,kd)\n",
    "    id2 = id1 + d\n",
    "    if id2 > population_size-1:\n",
    "        id2 = id2 - population_size-1\n",
    "            \n",
    "    # compare scores for the NN chains\n",
    "    # Put winner NN chain into W and loser NN chain into L\n",
    "   \n",
    "    if scores[id1] >= scores[id2]:\n",
    "        W = popul[id1]\n",
    "        L = popul[id2]\n",
    "        w_id = id1\n",
    "        l_id = id2\n",
    "\n",
    "\n",
    "    else:\n",
    "        W = popul[id2]\n",
    "        L = popul[id1]\n",
    "        w_id = id2\n",
    "        l_id = id1\n",
    "\n",
    "            \n",
    "    return(W,L,w_id,l_id)\n",
    "\n",
    "# cross the 'genetics' of the winner with the loser, randomly mutate the loser\n",
    "\n",
    "def mutate_loser(W,L,cross,mutate,l_id):\n",
    "    \n",
    "    # Switch out NNs in the loser's array for NNs in the winners array\n",
    "\n",
    "    for i in range(0,len(L)):\n",
    "        x = random.randint(0,100)\n",
    "        if x < cross:\n",
    "            L[i] = W[i]\n",
    "        #if x < mutate and l_id != scores.index(max(scores)):\n",
    "        if x < mutate:\n",
    "            if i == 0:\n",
    "                rand = (random.randint(1,5))\n",
    "                model_path = n2c_NN_path + '(' + str(rand) + ')'    \n",
    "                NNN = createNN('generator',True)\n",
    "                NNN.load_state_dict(torch.load(model_path,map_location = 'cuda'))                   \n",
    "                L[i] = NNN\n",
    "            else:\n",
    "                rand = (random.randint(1,5))\n",
    "                model_path = c2r_NN_path + '(' + str(rand) + ')'    \n",
    "                NNN = createNN('generator',True)\n",
    "                NNN.load_state_dict(torch.load(model_path,map_location = 'cuda'))                   \n",
    "                L[i] = NNN \n",
    "                \n",
    "    return L\n",
    "\n",
    "# start genetic algorithm loop\n",
    "\n",
    "def geneticAlgorithm(epochs,pop,scores,noise_data,chain_path,save_path,score_path,classifier):\n",
    "    \n",
    "    best_score = -100\n",
    "    kd = 1\n",
    "    \n",
    "    scorez = scores\n",
    "    best_chain_scores = []\n",
    "    best_chains = []\n",
    "    popu = pop\n",
    "    for i in range(1,epochs):\n",
    "        W,L,w_id,l_id = getWinnerLoser(popu,scorez,kd)\n",
    "\n",
    "    \n",
    "        mutated_L = mutate_loser(W,L,60,20,l_id)\n",
    "        popu[l_id] = mutated_L\n",
    "    \n",
    "        mutated_L_img_list = NNPass(noise_data, popu[l_id],chain_path)\n",
    "    \n",
    "        saveImgList(mutated_L_img_list,save_path,'')\n",
    "        \n",
    "    \n",
    "        mutated_L_score = scoreImgList(score_path,classifier,'grayscale')\n",
    "               \n",
    "        scorez[l_id] = mutated_L_score\n",
    "        \n",
    "        r = random.randint(0,100)\n",
    "        if kd < len(L)-1 and r < 9:\n",
    "            kd = kd+1\n",
    "            \n",
    "        display.clear_output(True)    \n",
    "        print('epoch',str(i),'-',max(scorez))\n",
    "            \n",
    "        best_score = max(scorez)\n",
    "    \n",
    "    return popu,best_chains,best_chain_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Network render function\n",
    "\n",
    "def multiNNRender(data_path,model_path,save_path,first,last,size):\n",
    "    \n",
    "    device_choice = 'cuda'\n",
    "      \n",
    "    dataset = createDataset(data_path,size,size,'color')\n",
    "\n",
    "    img_list = get_test_dataset(4,dataset)\n",
    "    \n",
    "    for i in range(first,last):\n",
    "        NN_path = model_path + str(i) + ')'\n",
    "        G = createNN('generator',False)\n",
    "        G.load_state_dict(torch.load(NN_path,map_location = device_choice))        \n",
    "        new_imgs =[]\n",
    "        \n",
    "        for j in img_list:\n",
    "            img = torch.tensor(j)\n",
    "            img2 = torch.stack((img,img))\n",
    "            img3 = G(img2.cuda()).data.cpu()\n",
    "            new_imgs.append(img3[0])      \n",
    "            \n",
    "        saveImgList(new_imgs,save_path,str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# App functions\n",
    "def useGenerator(img_data,model_path,save_path,name,gpu):\n",
    "    \n",
    "    new_imgs =[]   \n",
    "    img = torch.tensor(img_data[0])\n",
    "    img2 = torch.stack((img,img))  \n",
    "    \n",
    "    img3 = G2(img2)\n",
    "    \n",
    "    new_imgs.append(img3[0])\n",
    "\n",
    "    saveImgList(new_imgs,save_path,'')\n",
    "    \n",
    "def NNPath():\n",
    "    global NN_path\n",
    "    NN_path = filedialog.askopenfilename()\n",
    "\n",
    "    G2.load_state_dict(torch.load(NN_path,map_location = device))\n",
    "    w.create_rectangle(265,10,290,35, fill='lightgreen')\n",
    "    \n",
    "def imagePath():\n",
    "    global image_path\n",
    "    image_path = filedialog.askopenfilename()\n",
    "    \n",
    "    if image_path != '':\n",
    "        load = Image.open(image_path)\n",
    "        load = load.resize((260,260))\n",
    "        render = ImageTk.PhotoImage(load)\n",
    "        l = Label(image=render)\n",
    "        l.photo = render\n",
    "\n",
    "        l.place(x=19,y=59)\n",
    "        \n",
    "def createNewImage():\n",
    "    \n",
    "    if image_path is not '' and NN_path is not '':\n",
    "    \n",
    "        A_img = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        A = transform(A_img)\n",
    "        A1 = torch.stack((A,A))\n",
    "\n",
    "        useGenerator(A1,NN_path,save_path,'ok',False)\n",
    "        \n",
    "        load2 = Image.open(new_path)\n",
    "        load2 = load2.resize((260,260))\n",
    "        render2 = ImageTk.PhotoImage(load2)\n",
    "        l2 = Label(image=render2)\n",
    "        l2.photo = render2\n",
    "\n",
    "        l2.place(x=359,y=59)\n",
    "        \n",
    "def app(gpu):\n",
    "        \n",
    "    norm_layer = functools.partial(nn.BatchNorm2d, affine=True)   \n",
    "    use_dropout = not 'store_true'\n",
    "    G2 = Generator(norm_layer, use_dropout, n_blocks=9)\n",
    "    \n",
    "    if gpu:\n",
    "        device = torch.device('cuda')\n",
    "        G2.to('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "        \n",
    "    normal_weight_init(G2)    \n",
    "    main_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "    save_path = main_path+'/saved_images/save/1'\n",
    "    transform = get_transform(256,256,'color')    \n",
    "    new_path = main_path+'/saved_images/save/1img.jpg'\n",
    "    \n",
    "    return G2,device,transform,save_path,new_path     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_choice = 'app'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_choice == 'train':\n",
    "    \n",
    "    # Start training process\n",
    "\n",
    "    GAB,GBA,DA,DB = createModels()\n",
    "\n",
    "    optimizers = createOptimizers()\n",
    "\n",
    "    schedulers = [get_scheduler(optimizer) for optimizer in optimizers]\n",
    "\n",
    "    pool_size = 50\n",
    "    A_num_imgs = 0\n",
    "    A_imgs = []\n",
    "    B_num_imgs = 0\n",
    "    B_imgs = []\n",
    "\n",
    "    A_pool = [pool_size,A_num_imgs,A_imgs]\n",
    "\n",
    "    B_pool = [pool_size,B_num_imgs,B_imgs]\n",
    "\n",
    "    main_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "    save_model_path = main_path + '/models'\n",
    "    save_image_path = main_path + '/saved_images'\n",
    "    train_path = main_path+'/datasets/noise2color'\n",
    "\n",
    "    startTraining(50, 96, 64, 2,save_model_path,save_image_path,schedulers,optimizers)\n",
    "    \n",
    "elif run_choice == 'classifier':\n",
    "\n",
    "    # Create and save classifers\n",
    "    main_path = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "    data_path = main_path + '/datasets/faces'\n",
    "    save_classifier_path = main_path + '/models/classifiers'\n",
    "    createClassifier(3,data_path,save_classifier_path,64,'color')\n",
    "    \n",
    "elif run_choice == 'genetic algorithm':\n",
    "    \n",
    "    # noise2color and color2render model pathways\n",
    "\n",
    "    main_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "    n2c_NN_path = main_path+'/models/noise2color/n2c '\n",
    "\n",
    "    c2r_NN_path = main_path +'/models/color2render/c2r '\n",
    "\n",
    "    # noise data path\n",
    "\n",
    "    noise_data_path = main_path+'/datasets/single_noise'\n",
    "    noise = get_images(noise_data_path,256,'color')\n",
    "\n",
    "    #noise = createDataset(noise_data_path,256,256,'color')\n",
    "\n",
    "    reupload_path = main_path+'/GA/chain'\n",
    "    chain_path = main_path+'/GA/chain/trainA/1'\n",
    "\n",
    "    save_path = main_path+'/GA/score/trainA/1'\n",
    "    score_path = main_path+'/GA/score'\n",
    "\n",
    "    discriminator_path = main_path+'/models/classifiers/d (1)'\n",
    "    \n",
    "    # Start Genetic Algorithm Process\n",
    "\n",
    "    classifier = createNN('discriminator',True)\n",
    "    classifier.load_state_dict(torch.load(discriminator_path,map_location = 'cuda'))\n",
    "\n",
    "    pop,scores,pop_desc = createPopulation(noise,save_path,10,3)\n",
    "    popu,best_chains,best_chain_scores = geneticAlgorithm(100,pop,scores,noise,chain_path,save_path,score_path,classifier)\n",
    "    \n",
    "elif run_choice == 'multi render':\n",
    "    \n",
    "    # Start multiple NN render sequence\n",
    "    \n",
    "    main_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "    \n",
    "    model_path = main_path+'/models/noise2color/n2c ('\n",
    "    noise_data_path = main_path+'/datasets/single_noise'\n",
    "    save_path = main_path+'/saved_images'\n",
    "    multiNNRender(noise_data_path,model_path,save_path,1,5,64)\n",
    "    \n",
    "elif run_choice == 'app':\n",
    "    # GUI Creation -------------------------------------------\n",
    "\n",
    "    master = Tk()\n",
    "\n",
    "    w = Canvas(master, width=640, height=340)\n",
    "    w.pack()\n",
    "    image_path = ''\n",
    "    NN_path = ''\n",
    "\n",
    "    createNewImageButton = Button(master, text =\"Create New Image\", command = createNewImage,height = 1,padx = 5)\n",
    "    importImageButton = Button(master, text =\"Import Image\", command = imagePath,height = 1,padx = 5)\n",
    "    importNNButton = Button(master, text =\"Import Neural Network\", command = NNPath,height = 1,padx = 5)\n",
    "\n",
    "    importNNButton.place(x = 110,y=10)\n",
    "    importImageButton.place(x = 10,y=10)\n",
    "    createNewImageButton.place(x = 350,y=10)\n",
    "\n",
    "    w.create_rectangle(10,50,290,330, fill='tomato')\n",
    "    w.create_rectangle(350,50,630,330, fill='lightblue')\n",
    "    w.create_rectangle(300,180,340,190, fill='grey')\n",
    "    w.create_rectangle(265,10,290,35, fill='lightgrey')\n",
    "    \n",
    "    # Start app\n",
    "\n",
    "    G2,device,transform,save_path,new_path = app(False)\n",
    "    mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
