{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import functools\n",
    "from torch.optim import lr_scheduler\n",
    "import itertools\n",
    "from torch import optim \n",
    "import random \n",
    "import os \n",
    "from PIL import Image \n",
    "import torch.utils.data as data\n",
    "import os.path\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets,transforms,utils\n",
    "import torch.nn.functional as F \n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "from IPython import display\n",
    "import numpy\n",
    "import random\n",
    "from tkinter import *\n",
    "from tkinter.messagebox import showinfo\n",
    "import pdb\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data set creation functions\n",
    "\n",
    "# Function that creates a list of transforms from the Pytorch library\n",
    "# Each image in the dataset is passed through the transforms\n",
    "# Transforms include: Transfer to Tensor data structure, random crops, and normalization\n",
    "\n",
    "def get_transform(load_size,crop_size,color):\n",
    "    \n",
    "    transform_list = [transforms.Resize([load_size,load_size],Image.BICUBIC),transforms.RandomCrop(crop_size),\n",
    "                      transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))]\n",
    "    \n",
    "    if color == 'grayscale':\n",
    "        transform_list.append(transforms.Grayscale(num_output_channels = 3))\n",
    "        \n",
    "    composed_transforms = transforms.Compose(transform_list)\n",
    "    \n",
    "    return composed_transforms\n",
    "\n",
    "# Function that gets a chosen number of images from the dataset in order to display the network training progress\n",
    "\n",
    "def get_test_dataset(num_imgs,dataset):\n",
    "    a = 0\n",
    "    tensor_list_A = []\n",
    " \n",
    "    for i, data in enumerate(dataset):\n",
    " \n",
    "        real_A = data['A']\n",
    "\n",
    "        tensor_list_A.append(real_A[0])\n",
    "       \n",
    "        a +=1\n",
    "        if a == num_imgs:\n",
    "            break\n",
    "        \n",
    "    img_list = torch.stack(tensor_list_A) \n",
    "    return img_list\n",
    "\n",
    "# Class that defines the structure of the dataset\n",
    "# Sets up the file paths to each style domain image folder\n",
    "# Transforms and prepares each image for enumeration\n",
    "# Creates an indexing system for the dataset\n",
    "\n",
    "class DatasetStructure():\n",
    "    \n",
    "    def initialize(self,path,loadSize,fineSize,color):\n",
    "        \n",
    "        self.dir_A = os.path.join(path, 'train' + 'A')\n",
    "        self.dir_B = os.path.join(path, 'train' + 'B')\n",
    "        \n",
    "        self.A_paths = []\n",
    "    \n",
    "        for root, _, files in sorted(os.walk(self.dir_A)):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    path = os.path.join(root, file)\n",
    "                    self.A_paths.append(path)\n",
    "                    \n",
    "        self.B_paths = []\n",
    "    \n",
    "        for root, _, files in sorted(os.walk(self.dir_B)):\n",
    "            for file in files:\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    path = os.path.join(root, file)\n",
    "                    self.B_paths.append(path)                 \n",
    "\n",
    "        self.A_paths = sorted(self.A_paths)\n",
    "        self.B_paths = sorted(self.B_paths)\n",
    "        \n",
    "        self.A_size = len(self.A_paths)\n",
    "        self.B_size = len(self.B_paths)\n",
    "        \n",
    "        self.transform = get_transform(loadSize,fineSize,color) \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        A_path = self.A_paths[index % self.A_size]\n",
    "\n",
    "        index_B = index % self.B_size\n",
    "\n",
    "        B_path = self.B_paths[index_B]\n",
    "        A_img = Image.open(A_path).convert('RGB')\n",
    "        B_img = Image.open(B_path).convert('RGB')\n",
    "\n",
    "        A = self.transform(A_img)\n",
    "        B = self.transform(B_img)\n",
    "\n",
    "        return {'A': A, 'B': B,\n",
    "                'A_paths': A_path, 'B_paths': B_path}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(self.A_size, self.B_size)\n",
    "    \n",
    "# Creates an instance of the dataset\n",
    "# Specifies the image sizes in the dataset and whether they are gray or color\n",
    "\n",
    "def createDataset(path,load_size,crop_size,color):\n",
    "    \n",
    "    instance = DatasetStructure()\n",
    "    instance.initialize(path,load_size,crop_size,color)\n",
    "    \n",
    "    dataset = instance\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=not 'store_true', num_workers=int(0))\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def new_dataset(path,size,color):\n",
    "    \n",
    "    test_dataset = createDataset(path,size,size,color)\n",
    "\n",
    "    tensor_list = []    \n",
    "    for i, data in enumerate(test_dataset):\n",
    "        real_A = data['A']\n",
    "        real_B = data['B']\n",
    "        real = [real_A,real_B]\n",
    "        tensor_list.append(real[0][0])     \n",
    " \n",
    "    img_data = torch.stack(tensor_list)   \n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network functions\n",
    "\n",
    "# initializes NN weights\n",
    "\n",
    "def init_weights(net):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "\n",
    "            init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "            \n",
    "    net.apply(init_func)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        \n",
    "        NN_sequence = [nn.Conv2d(3,64,kernel_size=4,stride=2,padding=1),nn.LeakyReLU(0.2,True),\n",
    "                    \n",
    "                    nn.Conv2d(64, 64 * 2,kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "                    norm_layer(64 * 2),nn.LeakyReLU(0.2, True),\n",
    "                    \n",
    "                    nn.Conv2d(64 * 2, 64 * 4,kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "                    norm_layer(64 * 4),nn.LeakyReLU(0.2, True),\n",
    "                    \n",
    "                    nn.Conv2d(64 * 4, 64 * 8,kernel_size=4, stride=2, padding=1, bias=use_bias),\n",
    "                    norm_layer(64 * 8),nn.LeakyReLU(0.2, True),\n",
    "                    \n",
    "                    nn.Conv2d(64 * 8, 64 * 8,kernel_size=4, stride=1, padding=1, bias=use_bias),\n",
    "                    norm_layer(64 * 8),nn.LeakyReLU(0.2, True),\n",
    "                    \n",
    "                    nn.Conv2d(64 * 8, 1, kernel_size=4, stride=1, padding=1)]\n",
    "\n",
    "        if use_sigmoid:\n",
    "            NN_sequence += [nn.Sigmoid()]\n",
    "\n",
    "        self.model = nn.Sequential(*NN_sequence)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input) \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=9, padding_type='reflect'):\n",
    "        assert(n_blocks >= 0)\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d \n",
    "            \n",
    "        encoder = [nn.ReflectionPad2d(3),nn.Conv2d(3, 64, kernel_size=7, padding=0,bias=use_bias),norm_layer(64),nn.ReLU(True), \n",
    "                 \n",
    "                 nn.Conv2d(64, 64 * 2, kernel_size=3,stride=2, padding=1, bias=use_bias),\n",
    "                 norm_layer(64 * 2),nn.ReLU(True),\n",
    "                 \n",
    "                 nn.Conv2d(64 * 2, 64 * 4, kernel_size=3,stride=2, padding=1, bias=use_bias),\n",
    "                 norm_layer(64 * 4),nn.ReLU(True)]\n",
    "        \n",
    "        transformer = []\n",
    "        for i in range(n_blocks):\n",
    "            transformer += [ResnetBlock(64 * 4, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, \n",
    "                                  use_bias=use_bias)]           \n",
    "       \n",
    "        decoder = [nn.ConvTranspose2d(64 * 4, int(64 * 4 / 2),kernel_size=3, stride=2,padding=1, output_padding=1,\n",
    "                                      bias=use_bias),norm_layer(int(64 * 4 / 2)),nn.ReLU(True), \n",
    "                   \n",
    "                   nn.ConvTranspose2d(64 * 2, int(64 * 2 / 2),kernel_size=3, stride=2,padding=1, output_padding=1,\n",
    "                                      bias=use_bias),norm_layer(int(64 * 2 / 2)),nn.ReLU(True),\n",
    "                   \n",
    "                   nn.ReflectionPad2d(3),\n",
    "                   \n",
    "                   nn.Conv2d(64, 3, kernel_size=7, padding=0),\n",
    "                   \n",
    "                   nn.Tanh()]\n",
    "\n",
    "        model = encoder + transformer + decoder\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "    \n",
    "# Define a resnet block\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        \n",
    "        self.block = self.build_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "\n",
    "        block = [nn.ReflectionPad2d(1),\n",
    "                       \n",
    "                       nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=use_bias),norm_layer(dim),nn.ReLU(True)]\n",
    "        \n",
    "        if use_dropout:\n",
    "            block += [nn.Dropout(0.5)]\n",
    "\n",
    "        block += [nn.ReflectionPad2d(1),\n",
    "                       \n",
    "                  nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=use_bias),norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.block(x)\n",
    "        return out\n",
    "    \n",
    "def createModels():\n",
    "    norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
    "    \n",
    "    DA = Discriminator(norm_layer, 'store_true')\n",
    "    DA.to('cuda')  \n",
    "    init_weights(DA)\n",
    "    \n",
    "    DB = Discriminator(norm_layer, 'store_true')\n",
    "    DB.to('cuda')  \n",
    "    init_weights(DB)\n",
    "    \n",
    "    GAB = Generator(norm_layer, not 'store_true', n_blocks=9)\n",
    "    GAB.to('cuda')  \n",
    "    init_weights(GAB)\n",
    "    \n",
    "    GBA = Generator(norm_layer, not 'store_true', n_blocks=9)\n",
    "    GBA.to('cuda')  \n",
    "    init_weights(GBA)\n",
    "     \n",
    "    \n",
    "    return GAB,GBA,DA,DB\n",
    "\n",
    "def createNN(NN_type,gpu):\n",
    "    \n",
    "    norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n",
    "    \n",
    "    if NN_type == 'generator':\n",
    "        NN = Generator(norm_layer, not 'store_true', n_blocks=9)\n",
    "\n",
    "    elif NN_type == 'discriminator':\n",
    "        NN = Discriminator(norm_layer, 'store_true')\n",
    "      \n",
    "    if gpu:\n",
    "        NN.to('cuda')  \n",
    "    else:\n",
    "        NN.to('cpu')  \n",
    "        \n",
    "    init_weights(NN)\n",
    "        \n",
    "    return NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training functions\n",
    "\n",
    "def update_pool(imgs,pool):\n",
    "    pool_imgs = []\n",
    "    for img in imgs:\n",
    "        img = torch.unsqueeze(img.data, 0)\n",
    "        if pool[1] < pool[0]:\n",
    "            pool[1] = pool[1] + 1\n",
    "            pool[2].append(img)\n",
    "            pool_imgs.append(img)\n",
    "        else:\n",
    "            if random.uniform(0, 1) > 0.5:\n",
    "                r_id = random.randint(0, pool[0] - 1)\n",
    "                temp_img = pool[2][r_id].clone()\n",
    "                pool[2][r_id] = img\n",
    "                pool_imgs.append(temp_img)\n",
    "            else:\n",
    "                pool_imgs.append(img)\n",
    "    pool_imgs = torch.cat(pool_imgs, 0)\n",
    "    return pool_imgs\n",
    "\n",
    "def gan_loss(input,target):\n",
    "    \n",
    "    loss = nn.BCELoss()\n",
    "        \n",
    "    if target:\n",
    "        target_tensor = torch.tensor(1.0)\n",
    "    else:\n",
    "        target_tensor = torch.tensor(0.0)\n",
    "        \n",
    "    target_tensor = target_tensor.expand_as(input)\n",
    "    target_tensor = target_tensor.cuda()\n",
    "        \n",
    "    return loss(input,target_tensor)\n",
    "\n",
    "# learning rate functions\n",
    "\n",
    "def get_scheduler(optimizer):\n",
    "\n",
    "    def lambda_rule(epoch):\n",
    "        lr_l = 1.0 - max(0, epoch - 99) / float(101)\n",
    "        return lr_l\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "def update_learning_rate():\n",
    "    for scheduler in schedulers:\n",
    "        scheduler.step()\n",
    "    lr = optimizers[0].param_groups[0]['lr']\n",
    "    \n",
    "    \n",
    "def update_learning_rate1(schedulers,optimizers):\n",
    "    for scheduler in schedulers:\n",
    "        scheduler.step()\n",
    "    lr = optimizers[0].param_groups[0]['lr']\n",
    "    \n",
    "def createOptimizers():\n",
    "      \n",
    "    optimizer_G = torch.optim.Adam(itertools.chain(GAB.parameters(), GBA.parameters()),0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(itertools.chain(DA.parameters(), DB.parameters()),0.0002, betas=(0.5, 0.999))\n",
    "    optimizers = [optimizer_G,optimizer_D]\n",
    "    \n",
    "    return optimizers\n",
    "\n",
    "def optimize_parameters(real_A,real_B,optimizers):\n",
    "    \n",
    "    lf = torch.nn.L1Loss()\n",
    "    \n",
    "    fake_B = GAB(real_A)\n",
    "    re_A = GBA(fake_B)\n",
    "    fake_A = GBA(real_B)\n",
    "    re_B = GAB(fake_A)\n",
    "    \n",
    "    nets = [DA, DB]\n",
    "    requires_grad = False    \n",
    "    for net in nets:\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad = requires_grad           \n",
    "    optimizers[0].zero_grad()\n",
    "    \n",
    "    g_loss = (gan_loss(DA(fake_B), True)) + (gan_loss(DB(fake_A), True)) + (lf(re_A, real_A) * 10.0) + \\\n",
    "             (lf(re_B, real_B) * 10.0) + (lf(GAB(real_B), real_B) * 10.0 * 0.5) + \\\n",
    "             (lf(GBA(real_A), real_A) * 10.0 * 0.5)\n",
    "    \n",
    "    g_loss.backward()\n",
    "    \n",
    "    optimizers[0].step() \n",
    "        \n",
    "    nets = [DA, DB]\n",
    "    requires_grad = True   \n",
    "    for net in nets:\n",
    "        for param in net.parameters():\n",
    "            param.requires_grad = requires_grad            \n",
    "    optimizers[1].zero_grad()  \n",
    "    \n",
    "    # calculate discrimintor A loss, by predicting real images and generator AB fake images\n",
    "     \n",
    "    fake_B = update_pool(fake_B,B_pool)    \n",
    "    pred_real_DA = DA(real_B)\n",
    "    pred_fake_DA = DA(fake_B.detach())       \n",
    "    DA_loss = ((gan_loss(pred_real_DA, True)) + (gan_loss(pred_fake_DA, False))) * 0.5\n",
    "    DA_loss.backward() \n",
    "    \n",
    "    # calculate discrimintor B loss, by predicting real images and generator AB fake images    \n",
    "    \n",
    "    fake_A = update_pool(fake_A,A_pool)   \n",
    "    pred_real_DB = DB(real_A)    \n",
    "    pred_fake_DB = DB(fake_A.detach())    \n",
    "    DB_loss = ((gan_loss(pred_real_DB, True)) + (gan_loss(pred_fake_DB, False))) * 0.5\n",
    "    DB_loss.backward()\n",
    "    \n",
    "    optimizers[1].step()  \n",
    "    \n",
    "def startTraining(epoch_limit,in_size,out_size,sample_num,path,save_image_path):\n",
    "\n",
    "    # create dataset\n",
    "\n",
    "    dataset = createDataset(train_path,in_size,out_size,'color')\n",
    "\n",
    "    img_list = get_test_dataset(sample_num,dataset)\n",
    "        \n",
    "    # training process\n",
    "\n",
    "    for epoch in range(1,epoch_limit):\n",
    "        i = 0\n",
    "        for i, data in enumerate(dataset):\n",
    "        \n",
    "            real_A = data['A']\n",
    "            real_B = data['B']\n",
    "            if torch.cuda.is_available(): real_A = real_A.cuda()\n",
    "            if torch.cuda.is_available(): real_B = real_B.cuda()\n",
    "  \n",
    "            optimize_parameters(real_A,real_B,optimizers)\n",
    "        \n",
    "            # Display Progress\n",
    "            if (i) % 100 == 0:\n",
    "                display.clear_output(True)\n",
    "                # Display Images\n",
    "                test_images_B = GAB(img_list.cuda()).data.cpu()\n",
    "                        \n",
    "                saveImages(save_image_path,test_images_B,epoch,i,'cyclegan','test_save',i)\n",
    "\n",
    "                print (i)\n",
    "                print ('epoch: ' ,epoch,'/',200)     \n",
    "\n",
    "                saveModel(GAB,epoch,str(i),path)\n",
    "            i = i + 1\n",
    "        \n",
    "        update_learning_rate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ian_000\\Desktop\\GAN_final_code\n"
     ]
    }
   ],
   "source": [
    "main_path = os.path.dirname(os.path.abspath('__file__'))\n",
    "\n",
    "print(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training process\n",
    "\n",
    "GAB,GBA,DA,DB = createModels()\n",
    "\n",
    "optimizers = createOptimizers()\n",
    "\n",
    "schedulers = [get_scheduler(optimizer) for optimizer in optimizers]\n",
    "\n",
    "pool_size = 50\n",
    "A_num_imgs = 0\n",
    "A_imgs = []\n",
    "B_num_imgs = 0\n",
    "B_imgs = []\n",
    "\n",
    "A_pool = [pool_size,A_num_imgs,A_imgs]\n",
    "\n",
    "B_pool = [pool_size,B_num_imgs,B_imgs]\n",
    "\n",
    "save_model_path = 'F:/GAN/saved_models/test_models/'\n",
    "save_image_path = 'F:/GAN/saved_images/test'\n",
    "train_path = 'F:/GAN/datasets/color2render'\n",
    "\n",
    "startTraining(10, 96, 64, 1,save_model_path,save_image_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
